{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9670eb69",
   "metadata": {},
   "source": [
    "# Convert Dataset into Vectors\n",
    "The dataset we'll use combined 2 open source datasets curated by [The University of California, Irvine (UCI)](https://archive.ics.uci.edu). Learn how to [prepare this dataset here](https://github.com/codingforentrepreneurs/AI-Microservice-from-Scratch/blob/main/guides/Prepare%20the%20AI%20Spam%20Classifier%20Dataset.ipynb).\n",
    "\n",
    "#### Requirements\n",
    "- Python\n",
    "- Jupyter (Setup with [this video](https://www.youtube.com/watch?v=9tPS-7TWjq0))\n",
    "- Pandas\n",
    "- `scikit-learn` (aka `sklearn`)\n",
    "- Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f79271",
   "metadata": {},
   "source": [
    "### Step 1: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4add4a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dd5a8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PROJECT_ROOT = True\n",
    "BASE_DIR = pathlib.Path().resolve()\n",
    "if USE_PROJECT_ROOT:\n",
    "    BASE_DIR = BASE_DIR.parent.parent\n",
    "DATASET_DIR = BASE_DIR / \"datasets\"\n",
    "EXPORT_DIR = DATASET_DIR / \"exports\"\n",
    "DATASET_CSV_PATH = EXPORT_DIR / 'spam-dataset.csv'\n",
    "TRAINING_DATA_PATH = EXPORT_DIR / 'spam-training-data.pkl'\n",
    "\n",
    "\n",
    "if not DATASET_CSV_PATH.exists():\n",
    "    raise Exception(f\"You must download or create the spam-dataset.csv \\n{DATASET_CSV_PATH} not found.\")\n",
    "\n",
    "df = pd.read_csv(str(DATASET_CSV_PATH))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed519da",
   "metadata": {},
   "source": [
    "### Step 2: Convert Dataset to Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "991f6c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['text'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "\n",
    "labels_legend = {'ham': 0, 'spam': 1}\n",
    "labels_legend_inverted = {f\"{v}\":k for k,v in labels_legend.items()}\n",
    "\n",
    "labels_as_int = [labels_legend[str(x)] for x in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82205207",
   "metadata": {},
   "source": [
    "Now we need to map our labels from being text values to being integer values. It's pretty simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1dfdb966",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_legend = {'ham': 0, 'spam': 1}\n",
    "labels_legend_inverted = {f\"{v}\":k for k,v in labels_legend.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21da7e",
   "metadata": {},
   "source": [
    "The inverted legend is there to help us when we need to add a label to our predictions later.\n",
    "\n",
    "What's cool, is you can create this legend automatically with:\n",
    "\n",
    "```python\n",
    "legend = {f\"{x}\": i for i, x in enumerate(list(set(labels)))}\n",
    "legend_inverted = {f\"{v}\":k for k,v in legend.items()}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5fdc64dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_as_int =  [labels_legend[str(x)] for x in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009a8cae",
   "metadata": {},
   "source": [
    "__Verify the indices__\n",
    "\n",
    "It's important that our indices are still correct since this is the data we'll be using to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "612dbd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Index 4248\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random_idx = random.randint(0, len(texts))\n",
    "print('Random Index', random_idx)\n",
    "\n",
    "assert texts[random_idx] == df.iloc[random_idx].text\n",
    "assert labels[random_idx] == df.iloc[random_idx].label\n",
    "assert labels_legend_inverted[str(labels_as_int[random_idx])] == labels[random_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e0728f",
   "metadata": {},
   "source": [
    "### Step 3: Tokenize Texts\n",
    "\n",
    "The Keras Tokenizer will convert our raw text into vectors. Converting texts to vectors is a required step for any machine learning model (not just keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d99ec2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "! c:\\Users\\Acer\\Documents\\Visual-Studio\\MACHINE-LEARNING\\ai-api\\Scripts\\python.exe -m pip install tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "MAX_NUM_WORDS=280\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "58a5e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS=280"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b48b54",
   "metadata": {},
   "source": [
    "> `MAX_NUM_WORDS` is set to the current max length of any given post (tweet) on Twitter. This max number of words is likely to exceed *all* of our sms text size (typically 160 characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f62920ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9730 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1b0dc97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(sequences) == len(texts) == len(labels_as_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9cf179",
   "metadata": {},
   "source": [
    "### Step 4: Create `X`, `y` training sets\n",
    "\n",
    "In machine learning, it's common to denote the training inputs as `X` and their corresponding labels (the outputs) as `y`. \n",
    "\n",
    "Let's start with the `X` data (aka the text) by padding all of our tokenized sequences. This ensures all training inputs are the same shape (aka size). \n",
    "\n",
    "Each sentence in each paragraph in every conversation you have is rarely the same length. It is almost certainly *sometimes* the same length, but rarely all the time. With that in mind, we want to categorize every sentence (or paragraph) as either `spam` or `ham` -- an arbitrary length of data into known length of data. \n",
    "\n",
    "This means we have two challenges:\n",
    "- Matrix multiplication has strict rules\n",
    "- Spoken or written language rarely adheres to strict rules.\n",
    "\n",
    "What to do?\n",
    "\n",
    "`X` as new representation for the `text` from our raw dataset. As stated above, there's a very small chance that all data in this group is the exact same length so we'll use the built-in tool called `pad_sequences` to correct for the inconsistent length. This length is actually called shape because of it's roots in linear algebra (matrix multiplication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2d192107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 280\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "y = to_categorical(np.asarray(labels_as_int))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dd8f32",
   "metadata": {},
   "source": [
    "Now we covert our `labels_as_int` into a corresponding matrix value (instead of just a list of ints) by using the built-in `to_categorical` function. The number of labels does not have to be 2 (as we have) but it should be at least 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0b5115e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y = to_categorical(np.asarray(labels_as_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f37471",
   "metadata": {},
   "source": [
    "### Step 5: Split our Training Data\n",
    "\n",
    "If we trained on all of our data, our model will fit very *well* to that training data but it will not perform well on new data; aka it will be mostly useless.\n",
    "\n",
    "Since we have the `X` and `y` designations, we split the data into at least 2 corresponding sets: training data and validation data for each designation resulting in `X_train`, `X_test`, `y_train`, `y_test`.\n",
    "\n",
    "An easy way (but not the only way) is to use `scikit-learn` for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6b12e09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "! c:\\Users\\Acer\\Documents\\Visual-Studio\\MACHINE-LEARNING\\ai-api\\Scripts\\python.exe -m pip install sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969df478",
   "metadata": {},
   "source": [
    "As we'll see soon, the test sets (aka `X_test` and `y_test`) are used to evaluate how our AI model is learning (aka the performance). This means it's often a good idea to save the test sets for future training and not splitting the data all over again. Using the same test set over and over will show how our model is performing over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acbead2",
   "metadata": {},
   "source": [
    "### Step 6: Export our Training Data\n",
    "\n",
    "For this step, we'll use Python's built-in `pickle` module. Pickle is *not secure* so only open pickles that you create yourself. I am doing it as a way to pass data between jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2d107332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "training_data = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'max_words': MAX_NUM_WORDS,\n",
    "    'max_sequence': MAX_SEQUENCE_LENGTH,\n",
    "    'legend': labels_legend,\n",
    "    'labels_legend_inverted': labels_legend_inverted,\n",
    "    \"tokenizer\": tokenizer,\n",
    "}\n",
    "\n",
    "with open(TRAINING_DATA_PATH, 'wb') as f:\n",
    "    pickle.dump(training_data, f)\n",
    "\n",
    "data = {}\n",
    "\n",
    "with open(TRAINING_DATA_PATH, 'rb') as f:\n",
    "    data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1472deab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d1687e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5fd32e92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "eed96b006927a713996fdaa5880abc9e1be461c908ad8468a15439667a055f5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
